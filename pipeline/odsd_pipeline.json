{
  "pipelineSpec": {
    "components": {
      "comp-train-model": {
        "executorLabel": "exec-train-model",
        "inputDefinitions": {
          "parameters": {
            "bucket": {
              "type": "STRING"
            },
            "data_path": {
              "type": "STRING"
            },
            "glove_embedding_file": {
              "type": "STRING"
            },
            "model_name": {
              "type": "STRING"
            },
            "output_bucket": {
              "type": "STRING"
            },
            "save_path": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "Output": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-train-model": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "train_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-storage' 'pandas' 'sklearn' 'fsspec' 'gcsfs' 'nltk' 'tensorflow' 'tensorflowjs' 'kfp==1.8.12' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef train_model(\n  data_path: str,\n  bucket: str,\n  save_path: str,\n  model_name: str,\n  glove_embedding_file: str,\n  output_bucket: str,\n) -> str:\n  from google.cloud import storage\n\n  import pandas as pd\n  import numpy as np\n  import re\n  import string\n  import nltk\n  from sklearn.model_selection import train_test_split\n  import tensorflow as tf\n\n  import subprocess\n  import os\n\n  from nltk.corpus import stopwords\n\n  def nltk_init():\n    nltk.download('stopwords')\n    nltk.download('punkt')\n\n  def remove_stopwords(text):\n    stop_words = stopwords.words('english')\n    more_stopwords = ['u', 'im', 'c']\n    stop_words = stop_words + more_stopwords\n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n    return text\n\n  def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub(r'\\[.*?\\]', '', text)\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'<.*?>+', '', text)\n    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub(r'\\n', ' ', text)\n    text = re.sub(r'\\w*\\d\\w*', '', text)\n    return text\n\n  def stemm_text(text):\n    stemmer = nltk.SnowballStemmer(\"english\")\n    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n    return text\n\n  def upload_file_to_bucket(bucket_name: str, source: str, destination: str):\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination)\n    blob.upload_from_filename(source)\n\n  def read_file_from_bucket(bucket_name: str, file_path: str):\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(file_path)\n    return blob.download_as_string()\n\n  def process_df(df):\n    df['processed'] = df['message'].apply(clean_text)\n    df['processed'] = df['processed'].apply(remove_stopwords)\n    df['processed'] = df['processed'].apply(stemm_text)\n    return df\n\n  def load_data(file_path: str):\n    df = pd.read_csv(file_path, encoding=\"latin-1\")\n    df = df.dropna(how=\"any\", axis=1)\n    df = df.rename(columns={ \"v1\": \"target\", \"v2\": \"message\" })\n    return df\n\n  def encode_target(df):\n    df['target_encoded'] = df['target'].apply(lambda x: 1 if x == \"spam\" else 0)\n    return df\n\n  bucket_name = bucket.split('//')[-1] # Only use bucket name, without gs://\n  output_bucket_name = output_bucket.split('//')[-1] # Only use bucket name, without gs://\n  gcs_path = f\"{bucket}/{save_path}/{model_name}\"\n\n  nltk_init()\n\n  df = load_data(data_path)\n  df = process_df(df)\n  df = encode_target(df)\n\n  texts = df['processed']\n  target = df['target_encoded']\n\n  X_train, X_test, y_train, y_test = train_test_split(\n    texts, \n    target, \n    test_size=0.1,\n    random_state=42,\n  )\n\n  word_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n  word_tokenizer.fit_on_texts(X_train)\n\n  vocab_length = len(word_tokenizer.word_index) + 1\n  max_length = 80\n  metadata = {}\n  metadata[\"vocabulary_size\"] = vocab_length\n  metadata[\"max_length\"] = max_length\n\n  embeddings_dictionary = dict()\n  embedding_dim = 100\n\n  # Load GloVe 100D embeddings\n  embedding_txt = read_file_from_bucket(bucket_name, glove_embedding_file)\n  for line in embedding_txt.splitlines():\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\n\n  embedding_matrix = np.zeros((vocab_length, embedding_dim))\n\n  for word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n      embedding_matrix[index] = embedding_vector\n\n  def embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)\n\n  train_padded_sentences = tf.keras.preprocessing.sequence.pad_sequences(\n    embed(X_train), \n    max_length, \n    padding='post'\n  )\n\n  import json\n  word_index = {}\n  for word, index in word_tokenizer.word_index.items():\n    word_index[word] = index\n\n  metadata[\"word_index\"] = word_index\n\n  with open(\"metadata.json\", \"w\") as f:\n    json.dump(metadata, f)\n\n  def glove_lstm(max_length):\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input((max_length,)))\n    model.add(tf.keras.layers.Embedding(\n      input_dim=vocab_length,\n      output_dim=embedding_dim,\n      input_length=max_length,\n      weights = [embedding_matrix],\n      trainable=False\n    ))\n    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n      max_length, \n      return_sequences = True, \n      recurrent_dropout=0.2\n    )))\n    model.add(tf.keras.layers.GlobalMaxPool1D())\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(max_length, activation = \"relu\"))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(max_length, activation = \"relu\"))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\n    return model\n\n  model = glove_lstm(max_length)\n  test_padded_sentences = tf.keras.preprocessing.sequence.pad_sequences(\n    embed(X_test), \n    max_length, \n    padding='post'\n  )\n  model.fit(\n    train_padded_sentences, \n    y_train, \n    epochs = 7,\n    batch_size = 32,\n    validation_data = (test_padded_sentences, y_test),\n    verbose = 1,\n  )\n\n  if not os.path.exists('lstm_glove'):\n    os.mkdir('lstm_glove')\n\n  if not os.path.exists('tfjs_lstm_glove'):\n    os.mkdir('tfjs_lstm_glove')\n\n  model.save('lstm_glove')\n  os.listdir('lstm_glove')\n\n  subprocess.run([\"ls\", \"-l\"])\n\n  command = \"tensorflowjs_converter --input_format=tf_saved_model --output_format tfjs_graph_model --control_flow_v2=true ./lstm_glove ./tfjs_lstm_glove\".split(\" \")\n  subprocess.run(command)\n\n  upload_file_to_bucket(output_bucket_name, \"metadata.json\", f\"{save_path}/{model_name}/metadata.json\")\n  upload_file_to_bucket(output_bucket_name, \"tfjs_lstm_glove/model.json\", f\"{save_path}/{model_name}/model.json\")\n  upload_file_to_bucket(output_bucket_name, \"tfjs_lstm_glove/group1-shard1of1.bin\", f\"{save_path}/{model_name}/group1-shard1of1.bin\")\n  return gcs_path\n\n"
            ],
            "image": "python:3.9"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "odsd-pipeline"
    },
    "root": {
      "dag": {
        "tasks": {
          "train-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-model"
            },
            "inputs": {
              "parameters": {
                "bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://odsd"
                    }
                  }
                },
                "data_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://odsd/data/spam.csv"
                    }
                  }
                },
                "glove_embedding_file": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "data/glove.6B.100d.txt"
                    }
                  }
                },
                "model_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "odsd"
                    }
                  }
                },
                "output_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://odsd-public"
                    }
                  }
                },
                "save_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "model"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "train-model"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.12"
  },
  "runtimeConfig": {
    "gcsOutputDirectory": "gs://odsd/pipeline_root/odsd"
  }
}